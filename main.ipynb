{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to /Users/gregory/Code/TranscribeLeh/dataset/CloseMic/transcriptions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55213720defe42a798a2e726b35fdbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 01:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.473498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "import os\n",
    "import pandas as pd\n",
    "import tgt\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, TrainingArguments, Trainer\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "ROOT_PATH = os.getcwd()\n",
    "AUDIO_PATH = os.path.join(ROOT_PATH, \"dataset\", \"CloseMic\", \"audio\")\n",
    "SCRIPT_PATH = os.path.join(ROOT_PATH, \"dataset\", \"CloseMic\", \"scripts\")\n",
    "TRANSCRIPTION_DICT_PATH = os.path.join(ROOT_PATH, \"dataset\", \"CloseMic\", \"transcriptions.csv\")\n",
    "MODEL_BRAND = \"openai/whisper-tiny\"\n",
    "MODEL_PATH = os.path.join(ROOT_PATH, \"models\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "\twith open(file_path, \"rb\") as f:\n",
    "\t\traw_data = f.read(100000)\n",
    "\t\treturn chardet.detect(raw_data)[\"encoding\"]\n",
    "\n",
    "def extract_textgrid_transcriptions():\n",
    "\tdata = []\n",
    "\tfor filename in os.listdir(SCRIPT_PATH):\n",
    "\t\tif not filename.endswith(\".TextGrid\"): continue\n",
    "\n",
    "\t\ttextgrid_file = os.path.join(SCRIPT_PATH, filename)\n",
    "\t\taudio_file = filename.replace(\".TextGrid\", \".wav\")\n",
    "\t\taudio_path = os.path.join(AUDIO_PATH, audio_file)\n",
    "\n",
    "\t\tdetected_encoding = detect_encoding(textgrid_file)\n",
    "\t\tif not detected_encoding:\n",
    "\t\t\tprint(f\"Warning: Could not detect encoding for {filename}. Skipping.\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\twith open(textgrid_file, \"r\", encoding=detected_encoding) as f:\n",
    "\t\t\t\tcontent = f.read()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error reading {filename} with detected encoding '{detected_encoding}': {e}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tutf8_textgrid_file = textgrid_file + \".utf8\"\n",
    "\t\twith open(utf8_textgrid_file, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\tf.write(content)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\ttg = tgt.io.read_textgrid(utf8_textgrid_file, encoding=\"utf-8\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error parsing {filename}: {e}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Extract the correct tier name (matches filename)\n",
    "\t\ttier_name = filename.replace(\".TextGrid\", \"\")\n",
    "\t\tif tier_name not in [t.name for t in tg.tiers]:\n",
    "\t\t\tprint(f\"Warning: Tier '{tier_name}' not found in {filename}. Skipping.\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ttier = tg.get_tier_by_name(tier_name)\n",
    "\n",
    "\t\t# Extract transcription (ignoring special tokens like <S>, <SIL>, <Z>)\n",
    "\t\ttranscription = \" \".join(\n",
    "\t\t\t[interval.text for interval in tier.intervals if interval.text.strip() and interval.text not in {\"<S>\", \"<SIL>\", \"<Z>\"}]\n",
    "\t\t)\n",
    "\t\tif transcription: data.append({\"audio\": audio_path, \"text\": transcription})\n",
    "\t\tos.remove(utf8_textgrid_file)\n",
    "\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tdf.to_csv(TRANSCRIPTION_DICT_PATH, index=False)\n",
    "\tprint(f\"Saved dataset to {TRANSCRIPTION_DICT_PATH}\")\n",
    "\n",
    "def load_and_prepare_dataset():\n",
    "\tdf = pd.read_csv(TRANSCRIPTION_DICT_PATH)\n",
    "\tdf = df.head(2)  # Select only 2 samples for a quick test\n",
    "\tdataset = Dataset.from_pandas(df)\n",
    "\tdataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\tprocessor = WhisperProcessor.from_pretrained(MODEL_BRAND)\n",
    "\n",
    "\tdef preprocess(batch):\n",
    "\t\taudio = batch[\"audio\"]\n",
    "\t\tbatch[\"input_features\"] = processor(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "\t\ttokenized = processor.tokenizer(batch[\"text\"], truncation=True, max_length=448)\n",
    "\t\tbatch[\"labels\"] = tokenized.input_ids\n",
    "\t\treturn batch\n",
    "\n",
    "\tdataset = dataset.map(preprocess, remove_columns=[\"audio\"])\n",
    "\n",
    "\t# Split dataset\n",
    "\tdataset = dataset.train_test_split(test_size=0.2)  # 80% train, 20% eval\n",
    "\ttrain_dataset = dataset[\"train\"]\n",
    "\teval_dataset = dataset[\"test\"]\n",
    "\n",
    "\treturn train_dataset, eval_dataset, processor\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "\tprocessor: Any\n",
    "\tdef __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\t\tinput_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "\t\tlabel_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\t\tbatch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\t\tlabels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", padding=True)\n",
    "\t\tlabels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\t\tbatch[\"labels\"] = labels\n",
    "\t\treturn batch\n",
    "\n",
    "def fine_tune_whisper(train_dataset, eval_dataset, processor):\n",
    "\tmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_BRAND).to(DEVICE)\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\toutput_dir=MODEL_PATH,\n",
    "\t\tper_device_train_batch_size=2,\n",
    "\t\tmax_steps=2,\n",
    "\t\tevaluation_strategy=\"epoch\",\n",
    "\t\tsave_strategy=\"epoch\",\n",
    "\t\tlogging_dir=os.devnull,\n",
    "\t\treport_to=\"none\",\n",
    "\t\tlogging_strategy=\"no\",\n",
    "\t)\n",
    "\tdata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\ttrainer = Trainer(\n",
    "\t\tmodel=model,\n",
    "\t\targs=training_args,\n",
    "\t\ttrain_dataset=train_dataset,\n",
    "\t\teval_dataset=eval_dataset,\n",
    "\t\tprocessing_class=processor,\n",
    "\t\tdata_collator=data_collator,\n",
    "\t)\n",
    "\ttrainer.train()\n",
    "\ttrainer.save_model(MODEL_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\textract_textgrid_transcriptions()\n",
    "\ttrain_dataset, eval_dataset, processor = load_and_prepare_dataset()\n",
    "\tfine_tune_whisper(train_dataset, eval_dataset, processor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
