{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "import os\n",
    "import pandas as pd\n",
    "import tgt\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import EncoderDecoderCache, TrainingArguments, Trainer, WhisperForConditionalGeneration, WhisperProcessor\n",
    "from u import DataCollatorSpeechSeq2SeqWithPadding, detect_encoding\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "MODEL_BRAND = \"openai/whisper-tiny\"\n",
    "PROCESSOR = WhisperProcessor.from_pretrained(MODEL_BRAND)\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "AUDIO_PATH = os.path.join(ROOT_PATH, \"dataset\", \"CloseMic\", \"audio\")\n",
    "SCRIPTS_PATH = os.path.join(ROOT_PATH, \"dataset\", \"CloseMic\", \"scripts\")\n",
    "TRANSCRIPTIONS_PATH = os.path.join(ROOT_PATH, \"dataset\", \"CloseMic\", \"transcriptions.csv\")\n",
    "MODELS_PATH = os.path.join(ROOT_PATH, \"models\")\n",
    "INPUTS_PATH = os.path.join(ROOT_PATH, \"inputs\")\n",
    "\n",
    "def add_slang_tokens():\n",
    "\tspecial_tokens = [\"[lah]\"]\n",
    "\tPROCESSOR.tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "add_slang_tokens()\n",
    "\n",
    "def write_transcriptions():\n",
    "\ttranscriptions_list = []\n",
    "\n",
    "\tfor s in os.listdir(SCRIPT_PATH):\n",
    "\t\tscript_path = os.path.join(SCRIPT_PATH, s)\n",
    "\t\taudio_path = os.path.join(AUDIO_PATH, s.replace(\".TextGrid\", \".wav\"))\n",
    "\n",
    "\t\tscript_encoding = detect_encoding(script_path)\n",
    "\t\tif not script_encoding:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\twith open(script_path, \"r\", encoding=script_encoding) as f:\n",
    "\t\t\t\tscript_content = f.read()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tutf8_script_path = script_path + \".utf8\"\n",
    "\t\twith open(utf8_script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\tf.write(script_content)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tscript_textgrid_content = tgt.io.read_textgrid(utf8_script_path, encoding=\"utf-8\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ttier_name = s.replace(\".TextGrid\", \"\")\n",
    "\t\tif tier_name not in [t.name for t in script_textgrid_content.tiers]:\n",
    "\t\t\tcontinue\n",
    "\t\ttier = script_textgrid_content.get_tier_by_name(tier_name)\n",
    "\n",
    "\t\ttranscript = \" \".join([\n",
    "\t\t\tinterval.text for interval in tier.intervals if interval.text.strip() and interval.text not in {\"<S>\", \"<SIL>\", \"<Z>\"}\n",
    "\t\t])\n",
    "\t\tif not transcript:\n",
    "\t\t\tos.remove(utf8_script_path)\n",
    "\t\t\tcontinue\n",
    "\t\ttranscriptions_list.append({\"audio_path\": audio_path, \"transcript\": transcript})\n",
    "\t\tos.remove(utf8_script_path)\n",
    "\n",
    "\tdf = pd.DataFrame(transcriptions_list)\n",
    "\tdf.to_csv(TRANSCRIPTIONS_PATH, index=False)\n",
    "\n",
    "def load_dataset():\n",
    "\tdf = pd.read_csv(TRANSCRIPTIONS_PATH)\n",
    "\tdf = df.head(2)  # Remove only after team debugging and testing\n",
    "\tX = Dataset.from_pandas(df)\n",
    "\tX = X.cast_column(\"audio_path\", Audio(sampling_rate=16000))\n",
    "\n",
    "\tdef preprocess_batch(batch):\n",
    "\t\taudio = batch[\"audio_path\"]\n",
    "\t\tbatch[\"input_features\"] = PROCESSOR(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "\t\ttokenized = PROCESSOR.tokenizer(batch[\"transcript\"], truncation=True, max_length=448)\n",
    "\t\tbatch[\"labels\"] = tokenized.input_ids\n",
    "\t\treturn batch\n",
    "\n",
    "\tX = X.map(preprocess_batch, remove_columns=[\"audio_path\"])\n",
    "\tX = X.train_test_split(test_size=0.2)\n",
    "\treturn X[\"train\"], X[\"test\"], PROCESSOR\n",
    "\n",
    "def finetune_model(X_train, X_test):\n",
    "\tmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_BRAND).to(DEVICE)\n",
    "\tmodel.resize_token_embeddings(len(PROCESSOR.tokenizer))\n",
    "\tmodel.config.use_cache = False\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\toutput_dir=MODELS_PATH,\n",
    "\t\tper_device_train_batch_size=2,\n",
    "\t\tmax_steps=2,  # Replace as needed\n",
    "\t\teval_strategy=\"epoch\",\n",
    "\t\tsave_strategy=\"epoch\",\n",
    "\t\tlogging_dir=os.devnull,\n",
    "\t\treport_to=\"none\",\n",
    "\t\tlogging_strategy=\"no\",\n",
    "\t)\n",
    "\tdata_collator = DataCollatorSpeechSeq2SeqWithPadding(PROCESSOR=PROCESSOR)\n",
    "\ttrainer = Trainer(\n",
    "\t\tmodel=model,\n",
    "\t\targs=training_args,\n",
    "\t\ttrain_dataset=X_train,\n",
    "\t\teval_dataset=X_test,\n",
    "\t\tprocessing_class=PROCESSOR,\n",
    "\t\tdata_collator=data_collator,\n",
    "\t)\n",
    "\ttrainer.train()\n",
    "\ttrainer.save_model(MODEL_PATH)\n",
    "\n",
    "# write_transcriptions()\n",
    "# X_train, X_test = load_dataset()\n",
    "# finetune_model(X_train, X_test)\n",
    "\n",
    "def transcribe_audio(path):\n",
    "\tmodel = WhisperForConditionalGeneration.from_pretrained(\"models/checkpoint-2\").to(DEVICE)  # Replace as needed\n",
    "\taudio = Audio(sampling_rate=16000).decode_example({\"path\": path})\n",
    "\tinput_features = PROCESSOR(\n",
    "\t\taudio[\"array\"],\n",
    "\t\tsampling_rate=16000,\n",
    "\t\treturn_tensors=\"pt\"\n",
    "\t).input_features.to(DEVICE)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tpredicted_token_ids_tensor = model.generate(input_features)\n",
    "\treturn PROCESSOR.tokenizer.batch_decode(predicted_token_ids_tensor, skip_special_tokens=True)[0]\n",
    "\n",
    "def in_vocabulary(word):\n",
    "\ttokenized = PROCESSOR.tokenizer(word, add_special_tokens=False).input_ids\n",
    "\treturn len(tokenized) == 1 and tokenized[0] in PROCESSOR.tokenizer.get_vocab().values()\n",
    "\n",
    "print(in_vocabulary(\"[lah]\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
