{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 54863 new tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bde189760b4303a4fba951587cc258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataCollatorSpeechSeq2SeqWithPadding' object has no attribute 'MODEL_PROCESSOR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m write_transcriptions()\n\u001b[1;32m    131\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m load_dataset()\n\u001b[0;32m--> 132\u001b[0m finetune_model(X_train, X_test)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_audio\u001b[39m():\n\u001b[1;32m    135\u001b[0m \tmodel \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODELS_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-2\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "Cell \u001b[0;32mIn[5], line 127\u001b[0m, in \u001b[0;36mfinetune_model\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m    118\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorSpeechSeq2SeqWithPadding(processor\u001b[38;5;241m=\u001b[39mMODEL_PROCESSOR)\n\u001b[1;32m    119\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    120\u001b[0m \tmodel\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    121\u001b[0m \targs\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \tdata_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    126\u001b[0m )\n\u001b[0;32m--> 127\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    128\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(MODELS_PATH)\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2242\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2243\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2244\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2245\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2246\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/transformers/trainer.py:2500\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2498\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2499\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2500\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_samples(epoch_iterator, num_batches)\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2502\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/transformers/trainer.py:5180\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5180\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(epoch_iterator)]\n\u001b[1;32m   5181\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5182\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/accelerate/data_loader.py:564\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/Jupyter/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m, in \u001b[0;36mDataCollatorSpeechSeq2SeqWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     54\u001b[0m input_features \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m     55\u001b[0m label_features \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m---> 56\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMODEL_PROCESSOR\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mpad(input_features, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m labels_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMODEL_PROCESSOR\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(label_features, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmasked_fill(labels_batch\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataCollatorSpeechSeq2SeqWithPadding' object has no attribute 'MODEL_PROCESSOR'"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer, WhisperForConditionalGeneration, WhisperProcessor\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_BRAND = \"openai/whisper-tiny\"\n",
    "MODEL_PROCESSOR = WhisperProcessor.from_pretrained(MODEL_BRAND)\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "MODELS_PATH = os.path.join(ROOT_PATH, \"models\")\n",
    "LEXICON_PATH = os.path.join(ROOT_PATH, \"dataset\", \"lexicon.txt\")\n",
    "AUDIO_PATH = os.path.join(ROOT_PATH, \"dataset\", \"Channel0\", \"audio\")\n",
    "SCRIPTS_PATH = os.path.join(ROOT_PATH, \"dataset\", \"Channel0\", \"scripts\")\n",
    "TRANSCRIPTIONS_PATH = os.path.join(ROOT_PATH, \"dataset\", \"Channel0\", \"transcriptions.csv\")\n",
    "INPUTS_PATH = os.path.join(ROOT_PATH, \"inputs\")\n",
    "\n",
    "def add_new_tokens():\n",
    "\tglobal MODEL_PROCESSOR\n",
    "\tdataset_vocabulary_set = set()\n",
    "\twith open(LEXICON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\tfor lexicon_line in f:\n",
    "\t\t\tp = lexicon_line.strip().split(\"\\t\")\n",
    "\t\t\tif len(p) < 2:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tlexicon_word = p[0]\n",
    "\t\t\tdataset_vocabulary_set.add(lexicon_word)\n",
    "\n",
    "\tnovel_tokens_list = list(dataset_vocabulary_set - set(MODEL_PROCESSOR.tokenizer.get_vocab().keys()))\n",
    "\tif novel_tokens_list:\n",
    "\t\tprint(f\"Adding {len(novel_tokens_list)} new tokens\")\n",
    "\t\tMODEL_PROCESSOR.tokenizer.add_tokens(novel_tokens_list)\n",
    "\t\tMODEL_PROCESSOR.save_pretrained(MODELS_PATH)\n",
    "\t\tMODEL_PROCESSOR = WhisperProcessor.from_pretrained(MODELS_PATH)\n",
    "\telse:\n",
    "\t\tprint(\"No new tokens to add\")\n",
    "\n",
    "add_new_tokens()\n",
    "\n",
    "def detect_encoding(path):\n",
    "\twith open(path, \"rb\") as f:\n",
    "\t\treturn chardet.detect(f.read(100000))[\"encoding\"]\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "\tprocessor: Any\n",
    "\tdef __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\t\tinput_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "\t\tlabel_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\t\tbatch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\t\tlabels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", padding=True)\n",
    "\t\tlabels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\t\tbatch[\"labels\"] = labels\n",
    "\t\treturn batch\n",
    "\n",
    "def write_transcriptions():\n",
    "\ttranscriptions_list = []\n",
    "\n",
    "\tfor s in os.listdir(SCRIPTS_PATH):\n",
    "\t\tscript_path = os.path.join(SCRIPTS_PATH, s)\n",
    "\t\tscript_encoding = detect_encoding(script_path)\n",
    "\t\tif not script_encoding:\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\twith open(script_path, \"r\", encoding=script_encoding) as f:\n",
    "\t\t\t\tscript_lines = f.readlines()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tcontinue\n",
    "\t\tfor script_line in script_lines:\n",
    "\t\t\tx = script_line.strip().split(\"\\t\")\n",
    "\t\t\tif len(x) != 2:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tidentifier, transcript = x\n",
    "\t\t\taudio_path = os.path.join(AUDIO_PATH, f\"{identifier}.WAV\")\n",
    "\t\t\tif os.path.exists(audio_path):\n",
    "\t\t\t\ttranscriptions_list.append({\"audio_path\": audio_path, \"transcript\": transcript})\n",
    "\n",
    "\tdf = pd.DataFrame(transcriptions_list)\n",
    "\tdf.to_csv(TRANSCRIPTIONS_PATH, index=False)\n",
    "\n",
    "def load_dataset():\n",
    "\tdf = pd.read_csv(TRANSCRIPTIONS_PATH)\n",
    "\tdf = df.head(2)  # Remove only after team debugging and testing\n",
    "\tX = Dataset.from_pandas(df)\n",
    "\tX = X.cast_column(\"audio_path\", Audio(sampling_rate=16000))\n",
    "\n",
    "\tdef preprocess_batch(batch):\n",
    "\t\taudio = batch[\"audio_path\"]\n",
    "\t\tbatch[\"input_features\"] = MODEL_PROCESSOR(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "\t\ttokenized = MODEL_PROCESSOR.tokenizer(batch[\"transcript\"], truncation=True, max_length=448)\n",
    "\t\tbatch[\"labels\"] = tokenized.input_ids\n",
    "\t\treturn batch\n",
    "\n",
    "\tX = X.map(preprocess_batch, remove_columns=[\"audio_path\"])\n",
    "\tX = X.train_test_split(test_size=0.2)\n",
    "\treturn X[\"train\"], X[\"test\"]\n",
    "\n",
    "def finetune_model(X_train, X_test):\n",
    "\tmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_BRAND).to(DEVICE)\n",
    "\tmodel.resize_token_embeddings(len(MODEL_PROCESSOR.tokenizer))\n",
    "\tmodel.config.use_cache = False\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\toutput_dir=MODELS_PATH,\n",
    "\t\tper_device_train_batch_size=2,\n",
    "\t\tmax_steps=2,  # Replace as needed\n",
    "\t\teval_strategy=\"epoch\",\n",
    "\t\tsave_strategy=\"epoch\",\n",
    "\t\tlogging_dir=os.devnull,\n",
    "\t\treport_to=\"none\",\n",
    "\t\tlogging_strategy=\"no\",\n",
    "\t)\n",
    "\tdata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=MODEL_PROCESSOR)\n",
    "\ttrainer = Trainer(\n",
    "\t\tmodel=model,\n",
    "\t\targs=training_args,\n",
    "\t\ttrain_dataset=X_train,\n",
    "\t\teval_dataset=X_test,\n",
    "\t\tprocessing_class=MODEL_PROCESSOR,\n",
    "\t\tdata_collator=data_collator,\n",
    "\t)\n",
    "\ttrainer.train()\n",
    "\ttrainer.save_model(MODELS_PATH)\n",
    "\n",
    "write_transcriptions()\n",
    "X_train, X_test = load_dataset()\n",
    "finetune_model(X_train, X_test)\n",
    "\n",
    "def transcribe_audio():\n",
    "\tmodel = WhisperForConditionalGeneration.from_pretrained(os.path.join(MODELS_PATH, \"checkpoint-2\")).to(DEVICE)\n",
    "\tfor a in [f for f in os.listdir(INPUTS_PATH) if f.lower().endswith((\".wav\", \".mp3\", \".flac\"))]:\n",
    "\t\tinput_audio_path = os.path.join(INPUTS_PATH, a)\n",
    "\t\tinput_audio = Audio(sampling_rate=16000).decode_example({\"path\": input_audio_path})\n",
    "\t\tinput_audio_features = MODEL_PROCESSOR(\n",
    "\t\t\taudio[\"array\"],\n",
    "\t\t\tsampling_rate=16000,\n",
    "\t\t\treturn_tensors=\"pt\"\n",
    "\t\t).input_features.to(DEVICE)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpredicted_token_ids_tensor = model.generate(input_audio_features)\n",
    "\t\ttranscription = MODEL_PROCESSOR.tokenizer.batch_decode(predicted_token_ids_tensor, skip_special_tokens=True)[0]\n",
    "\t\tprint(f\"{input_audio_path}: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_vocabulary(word):\n",
    "\ttokenized = MODEL_PROCESSOR.tokenizer(word, add_special_tokens=False).input_ids\n",
    "\treturn len(tokenized) == 1 and tokenized[0] in MODEL_PROCESSOR.tokenizer.get_vocab().values()\n",
    "\n",
    "print(in_vocabulary(\"lah\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
